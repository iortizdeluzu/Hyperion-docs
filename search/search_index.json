{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"1_introduction/","text":"Introduction In the modern era of research, High-Performance Computing (HPC) has emerged as a cornerstone, powering groundbreaking innovations and discoveries. It provides the robust computational capacity necessary for handling highly complex and data-intensive tasks. Whether it's climate modeling, genomic research, artificial intelligence, or particle physics, HPC is the engine driving these high-stake computations. However, to maximize the utility of an HPC system and ensure its resources are used efficiently, we require a mechanism that can manage, allocate, and schedule computational tasks. This is where workload managers, or job schedulers, step in. Role and Importance of Workload Managers Workload managers orchestrate the computational tasks, also known as jobs, on the computing nodes within an HPC cluster. They are the conductors of the HPC symphony, optimizing the allocation of the system's resources. Without a workload manager: Users would have to manually assign their jobs to specific nodes and make sure those nodes are not already in use. This is not only inefficient but also greatly increases the chance of error and system underutilization. There would be no mechanism to queue jobs. Hence, users would have to constantly monitor the system and manually launch jobs once resources become available. Handling the priorities for different jobs would be a nightmare. Some users might unfairly monopolize resources, while others might be left waiting for an unreasonably long time. In contrast, a workload manager automates these tasks and provides several significant advantages: Efficiency : It automates the job scheduling process, selecting the most suitable resources based on the job's requirements and the scheduling policies. This leads to increased system utilization and decreased job waiting times. Fairness : It manages job priorities based on factors such as user quotas and job sizes, ensuring that all users get their fair share of the system resources. Ease of Use : It provides users with commands to submit jobs, check their status, and cancel them if necessary. This makes it much easier for users to interact with the HPC system. Flexibility : It allows system administrators to implement policies that govern job priorities, system utilization, and resource allocation, providing a high degree of control over the system's operation. Why SLURM? Among the plethora of workload managers, SLURM (Simple Linux Utility for Resource Management) stands out and is widely adopted in the HPC community. Here are the compelling reasons for choosing SLURM: Scalability and Performance : SLURM is built to scale and is capable of managing scheduling for some of the world's largest supercomputers, making it an excellent choice regardless of the size of the HPC system. Flexibility and Configurability : SLURM boasts high flexibility, providing numerous options that can be tweaked to match the specific needs of a particular system or set of users. Advanced Resource Management : SLURM supports a wide variety of resource types and allows for complex resource selection, ensuring that jobs receive the resources they need. Open Source and Active Development : Being open-source, SLURM benefits from the collective wisdom and efforts of a global community. It's continually improved, debugged, and updated, ensuring users have access to the latest features and performance improvements. Robustness and Reliability : SLURM provides automatic failover and fault-tolerant job management capabilities, ensuring the continuity of operations even when individual components fail. By the end of this course, we aim for you to be comfortable with using SLURM for managing your computational tasks, ranging from the creation and submission of jobs, monitoring their execution, to managing output files and analyzing job","title":"Introduction"},{"location":"1_introduction/#introduction","text":"In the modern era of research, High-Performance Computing (HPC) has emerged as a cornerstone, powering groundbreaking innovations and discoveries. It provides the robust computational capacity necessary for handling highly complex and data-intensive tasks. Whether it's climate modeling, genomic research, artificial intelligence, or particle physics, HPC is the engine driving these high-stake computations. However, to maximize the utility of an HPC system and ensure its resources are used efficiently, we require a mechanism that can manage, allocate, and schedule computational tasks. This is where workload managers, or job schedulers, step in.","title":"Introduction"},{"location":"1_introduction/#role-and-importance-of-workload-managers","text":"Workload managers orchestrate the computational tasks, also known as jobs, on the computing nodes within an HPC cluster. They are the conductors of the HPC symphony, optimizing the allocation of the system's resources. Without a workload manager: Users would have to manually assign their jobs to specific nodes and make sure those nodes are not already in use. This is not only inefficient but also greatly increases the chance of error and system underutilization. There would be no mechanism to queue jobs. Hence, users would have to constantly monitor the system and manually launch jobs once resources become available. Handling the priorities for different jobs would be a nightmare. Some users might unfairly monopolize resources, while others might be left waiting for an unreasonably long time. In contrast, a workload manager automates these tasks and provides several significant advantages: Efficiency : It automates the job scheduling process, selecting the most suitable resources based on the job's requirements and the scheduling policies. This leads to increased system utilization and decreased job waiting times. Fairness : It manages job priorities based on factors such as user quotas and job sizes, ensuring that all users get their fair share of the system resources. Ease of Use : It provides users with commands to submit jobs, check their status, and cancel them if necessary. This makes it much easier for users to interact with the HPC system. Flexibility : It allows system administrators to implement policies that govern job priorities, system utilization, and resource allocation, providing a high degree of control over the system's operation.","title":"Role and Importance of Workload Managers"},{"location":"1_introduction/#why-slurm","text":"Among the plethora of workload managers, SLURM (Simple Linux Utility for Resource Management) stands out and is widely adopted in the HPC community. Here are the compelling reasons for choosing SLURM: Scalability and Performance : SLURM is built to scale and is capable of managing scheduling for some of the world's largest supercomputers, making it an excellent choice regardless of the size of the HPC system. Flexibility and Configurability : SLURM boasts high flexibility, providing numerous options that can be tweaked to match the specific needs of a particular system or set of users. Advanced Resource Management : SLURM supports a wide variety of resource types and allows for complex resource selection, ensuring that jobs receive the resources they need. Open Source and Active Development : Being open-source, SLURM benefits from the collective wisdom and efforts of a global community. It's continually improved, debugged, and updated, ensuring users have access to the latest features and performance improvements. Robustness and Reliability : SLURM provides automatic failover and fault-tolerant job management capabilities, ensuring the continuity of operations even when individual components fail. By the end of this course, we aim for you to be comfortable with using SLURM for managing your computational tasks, ranging from the creation and submission of jobs, monitoring their execution, to managing output files and analyzing job","title":"Why SLURM?"},{"location":"2_workflow/","text":"Workflow The typical workflow of a researcher using SLURM in an HPC environment involves several steps: Connect to the System : Using Secure Shell (SSH), you connect to the login node of the HPC cluster. Prepare Your Work : This might include copying input files, writing scripts, or compiling programs. Job Submission : You write a batch script for SLURM that outlines the requirements and commands for your job, and submit this script to the scheduler. Monitor Your Job : You can keep track of your job's progress using several SLURM commands. Transfer Results : Once your job is completed, you can copy your output files from the compute node back to your local system. In the following sections, we will delve into each of these steps, starting with a detailed walkthrough of a simple example.","title":"Workflow"},{"location":"2_workflow/#workflow","text":"The typical workflow of a researcher using SLURM in an HPC environment involves several steps: Connect to the System : Using Secure Shell (SSH), you connect to the login node of the HPC cluster. Prepare Your Work : This might include copying input files, writing scripts, or compiling programs. Job Submission : You write a batch script for SLURM that outlines the requirements and commands for your job, and submit this script to the scheduler. Monitor Your Job : You can keep track of your job's progress using several SLURM commands. Transfer Results : Once your job is completed, you can copy your output files from the compute node back to your local system. In the following sections, we will delve into each of these steps, starting with a detailed walkthrough of a simple example.","title":"Workflow"},{"location":"3_first-job/","text":"First Example: Estimation of the valur of Pi with Monte Carlo methods Before delving into the code, let's further explore the problem at hand. Pi (\u03c0) is a mathematical constant originally defined as the ratio of a circle's circumference to its diameter. It's a fundamental element in mathematics and appears in many formulas in all areas of mathematics and physics. However, \u03c0 is an irrational number, meaning it cannot be expressed as a simple fraction, and its decimal representation never ends or settles into a permanently repeating pattern. Although we usually approximate \u03c0 as 3.14159, its exact value is unknown. So how can we estimate it using a computer program? That's where the Monte Carlo method comes in. The Monte Carlo Method The Monte Carlo method, named after the famous Monaco casino, is a statistical technique that uses random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. In this case, we'll be using the Monte Carlo method to estimate the value of \u03c0. Here's the idea: Create a bounded area : Imagine a square with a side length of 1 unit. Now, inscribe a quarter-circle with a radius of 1 unit inside this square, like a pie wedge. Throw darts : We then \"throw darts\" at this square. The position (x, y) of each dart is determined randomly. Determine if the dart is inside the quarter-circle : For each dart, we compute whether it has landed inside the quarter-circle using the equation of a circle (x^2 + y^2 < r^2, where r is the radius). If the dart lands inside the circle (x^2 + y^2 < 1), we consider it a hit. Approximate \u03c0 : We do this thousands or even millions of times. The ratio of the number of darts that hit inside the circle to the total number of darts thrown will approximately be \u03c0/4. So, by multiplying this ratio by 4, we can approximate \u03c0! Now, this process is inherently parallel \u2014 each \"dart throw\" is an independent event, and we can perform multiple dart throws simultaneously. This is a perfect scenario for using a high-performance computing environment like SLURM. The more parallel processes we can run (and hence, the more darts we can throw), the better our approximation of \u03c0 can be. In the example script, we'll be generating these \"random dart throws\" using the Bash $RANDOM variable and then estimating \u03c0 in parallel tasks. By submitting this task to SLURM, we'll effectively be demonstrating a simple, yet powerful use case of HPC. Absolutely, we can integrate the use of sbatch , salloc , and srun into the Monte Carlo Pi estimation example. Here's the revised section: Sample Batch Script First, let's examine the SLURM batch script. We'll name our batch script pi_estimation.sh . It's important to note that the batch script is the primary way to submit jobs to the SLURM scheduler, and we submit this script using the sbatch command. #!/bin/bash #SBATCH --job-name=PiEstimation #SBATCH --output=pi_job.%j.out #SBATCH --error=pi_job.%j.err #SBATCH --nodes=1 #SBATCH --ntasks-per-node=4 #SBATCH --time=01:00:00 #SBATCH --mail-type=END,FAIL #SBATCH --mail-user=your-email@example.com # Load any necessary modules and activate the conda environment (if any) # module load python/3.8 # Start the job steps date;hostname;pwd for i in $(seq 1 $SLURM_NTASKS); do ( # Each task performs 10^6 trials trials=1000000 count=0 for (( j=0; j<trials; j++ )); do x=$(echo \"scale=4; $RANDOM/32767\" | bc -l) y=$(echo \"scale=4; $RANDOM/32767\" | bc -l) # Test if the point ($x, $y) lies within the unit circle inside=$( echo \"$x*$x + $y*$y < 1.0\" | bc -l) # Increase count if the point is inside the circle if [[ $inside -eq 1 ]]; then ((count++)) fi done # Estimate Pi based on the count and the number of trials pi=$(echo \"scale=4; 4*$count/$trials\" | bc -l) echo \"Task $i: Pi is approximately $pi\" ) & done # Wait for all background tasks to finish wait Once you've written your batch script, you can submit it to SLURM using the sbatch command. The sbatch command reads a script file and submits the script as a job to SLURM. The script contains both the job parameters, specified on lines beginning with #SBATCH , and the job commands. sbatch pi_estimation.sh Here are the explanations for each SBATCH directive used in our batch script: Directive Explanation --job-name=PiEstimation Sets the name of the job. This name appears in the job listings, making it easier to manage jobs. --output=pi_job.%j.out Sets the name of the standard output file for the job. %j is replaced with the job ID. --error=pi_job.%j.err Sets the name of the standard error file for the job. %j is replaced with the job ID. --nodes=1 Specifies that the job requires 1 node. --ntasks-per-node=4 Specifies the number of tasks to be initiated on each node. In this case, we have 4 tasks per node. --time=01:00:00 Sets a limit on the total run time of the job. The job will be terminated if it runs longer than this limit. --mail-type=END,FAIL Sends an email when the job ends or fails. --mail-user=your-email@example.com Specifies the email address to receive job status updates. Remember that all SBATCH directives must come before any executable line in your script. This batch script generates four parallel tasks, each performing a million trials to estimate the value of Pi. It demonstrates both the basic usage of SLURM and the application of parallel processing in solving computationally intensive problems. To submit this job script to SLURM, we would use the sbatch command: sbatch pi_estimation.sh This script, as well as subsequent outputs, error logs, and other relevant files, will be used throughout this course as we delve deeper into the powerful features of SLURM. Interactive job with salloc In some cases, you might want to run your jobs interactively, that is, get a shell on a compute node where you can type commands and run programs directly. This can be done using salloc . Here's how you could use salloc to start an interactive shell with 4 CPUs for one hour, and then run the Pi estimation program interactively: salloc --ntasks=4 --time=01:00:00 bash pi_estimation.sh exit The salloc command allocates resources (in this case, 4 tasks for a duration of one hour) and starts a shell. In that shell, you can then directly execute the pi_estimation.sh script. Once you're done, don't forget to type exit to release the allocation. Direct job step execution with srun srun is another important command in SLURM. It allows you to run job steps directly without having to write a batch script. A job step is essentially a set of (possibly multiple) tasks that are co-scheduled across one or more nodes. In the context of our Pi estimation program, you could use srun to directly run the Pi estimation commands as a job step. However, as our script is a bit more complex (with loops and conditionals), it's not straightforward to do it with srun directly. Instead, we can wrap our core script into another script and then use srun to execute it: First, extract the core logic of our Pi estimation into a separate script, core_pi.sh : #!/bin/bash # Each task performs 10^6 trials trials=1000000 count=0 for (( j=0; j<trials; j++ )); do x=$(echo \"scale=4; $RANDOM/32767\" | bc -l) y=$(echo \"scale=4; $RANDOM/32767\" | bc -l) # Test if the point ($x, $y) lies within the unit circle inside=$( echo \"$x*$x + $y*$y < 1.0\" | bc -l) # Increase count if the point is inside the circle if [[ $inside -eq 1 ]]; then ((count++)) fi done # Estimate Pi based on the count and the number of trials pi=$(echo \"scale=4; 4*$count/$trials\" | bc -l) echo \"Task $SLURM_PROCID: Pi is approximately $pi\" Then, use srun to run this script as a job step, creating 4 tasks: srun --ntasks=4 --time=01:00:00 bash core_pi.sh Each of the 4 tasks will execute core_pi.sh separately, effectively running our Pi estimation in parallel. Remember, srun and salloc provide you with more flexibility and control over your job execution. You'll typically use sbatch for most of your jobs (especially long ones or ones that you want to schedule and forget), but srun and salloc can be very handy for quick or interactive jobs.","title":"First Job"},{"location":"3_first-job/#first-example-estimation-of-the-valur-of-pi-with-monte-carlo-methods","text":"Before delving into the code, let's further explore the problem at hand. Pi (\u03c0) is a mathematical constant originally defined as the ratio of a circle's circumference to its diameter. It's a fundamental element in mathematics and appears in many formulas in all areas of mathematics and physics. However, \u03c0 is an irrational number, meaning it cannot be expressed as a simple fraction, and its decimal representation never ends or settles into a permanently repeating pattern. Although we usually approximate \u03c0 as 3.14159, its exact value is unknown. So how can we estimate it using a computer program? That's where the Monte Carlo method comes in.","title":"First Example: Estimation of the valur of Pi with Monte Carlo methods"},{"location":"3_first-job/#the-monte-carlo-method","text":"The Monte Carlo method, named after the famous Monaco casino, is a statistical technique that uses random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. In this case, we'll be using the Monte Carlo method to estimate the value of \u03c0. Here's the idea: Create a bounded area : Imagine a square with a side length of 1 unit. Now, inscribe a quarter-circle with a radius of 1 unit inside this square, like a pie wedge. Throw darts : We then \"throw darts\" at this square. The position (x, y) of each dart is determined randomly. Determine if the dart is inside the quarter-circle : For each dart, we compute whether it has landed inside the quarter-circle using the equation of a circle (x^2 + y^2 < r^2, where r is the radius). If the dart lands inside the circle (x^2 + y^2 < 1), we consider it a hit. Approximate \u03c0 : We do this thousands or even millions of times. The ratio of the number of darts that hit inside the circle to the total number of darts thrown will approximately be \u03c0/4. So, by multiplying this ratio by 4, we can approximate \u03c0! Now, this process is inherently parallel \u2014 each \"dart throw\" is an independent event, and we can perform multiple dart throws simultaneously. This is a perfect scenario for using a high-performance computing environment like SLURM. The more parallel processes we can run (and hence, the more darts we can throw), the better our approximation of \u03c0 can be. In the example script, we'll be generating these \"random dart throws\" using the Bash $RANDOM variable and then estimating \u03c0 in parallel tasks. By submitting this task to SLURM, we'll effectively be demonstrating a simple, yet powerful use case of HPC. Absolutely, we can integrate the use of sbatch , salloc , and srun into the Monte Carlo Pi estimation example. Here's the revised section:","title":"The Monte Carlo Method"},{"location":"3_first-job/#sample-batch-script","text":"First, let's examine the SLURM batch script. We'll name our batch script pi_estimation.sh . It's important to note that the batch script is the primary way to submit jobs to the SLURM scheduler, and we submit this script using the sbatch command. #!/bin/bash #SBATCH --job-name=PiEstimation #SBATCH --output=pi_job.%j.out #SBATCH --error=pi_job.%j.err #SBATCH --nodes=1 #SBATCH --ntasks-per-node=4 #SBATCH --time=01:00:00 #SBATCH --mail-type=END,FAIL #SBATCH --mail-user=your-email@example.com # Load any necessary modules and activate the conda environment (if any) # module load python/3.8 # Start the job steps date;hostname;pwd for i in $(seq 1 $SLURM_NTASKS); do ( # Each task performs 10^6 trials trials=1000000 count=0 for (( j=0; j<trials; j++ )); do x=$(echo \"scale=4; $RANDOM/32767\" | bc -l) y=$(echo \"scale=4; $RANDOM/32767\" | bc -l) # Test if the point ($x, $y) lies within the unit circle inside=$( echo \"$x*$x + $y*$y < 1.0\" | bc -l) # Increase count if the point is inside the circle if [[ $inside -eq 1 ]]; then ((count++)) fi done # Estimate Pi based on the count and the number of trials pi=$(echo \"scale=4; 4*$count/$trials\" | bc -l) echo \"Task $i: Pi is approximately $pi\" ) & done # Wait for all background tasks to finish wait Once you've written your batch script, you can submit it to SLURM using the sbatch command. The sbatch command reads a script file and submits the script as a job to SLURM. The script contains both the job parameters, specified on lines beginning with #SBATCH , and the job commands. sbatch pi_estimation.sh Here are the explanations for each SBATCH directive used in our batch script: Directive Explanation --job-name=PiEstimation Sets the name of the job. This name appears in the job listings, making it easier to manage jobs. --output=pi_job.%j.out Sets the name of the standard output file for the job. %j is replaced with the job ID. --error=pi_job.%j.err Sets the name of the standard error file for the job. %j is replaced with the job ID. --nodes=1 Specifies that the job requires 1 node. --ntasks-per-node=4 Specifies the number of tasks to be initiated on each node. In this case, we have 4 tasks per node. --time=01:00:00 Sets a limit on the total run time of the job. The job will be terminated if it runs longer than this limit. --mail-type=END,FAIL Sends an email when the job ends or fails. --mail-user=your-email@example.com Specifies the email address to receive job status updates. Remember that all SBATCH directives must come before any executable line in your script. This batch script generates four parallel tasks, each performing a million trials to estimate the value of Pi. It demonstrates both the basic usage of SLURM and the application of parallel processing in solving computationally intensive problems. To submit this job script to SLURM, we would use the sbatch command: sbatch pi_estimation.sh This script, as well as subsequent outputs, error logs, and other relevant files, will be used throughout this course as we delve deeper into the powerful features of SLURM.","title":"Sample Batch Script"},{"location":"3_first-job/#interactive-job-with-salloc","text":"In some cases, you might want to run your jobs interactively, that is, get a shell on a compute node where you can type commands and run programs directly. This can be done using salloc . Here's how you could use salloc to start an interactive shell with 4 CPUs for one hour, and then run the Pi estimation program interactively: salloc --ntasks=4 --time=01:00:00 bash pi_estimation.sh exit The salloc command allocates resources (in this case, 4 tasks for a duration of one hour) and starts a shell. In that shell, you can then directly execute the pi_estimation.sh script. Once you're done, don't forget to type exit to release the allocation.","title":"Interactive job with salloc"},{"location":"3_first-job/#direct-job-step-execution-with-srun","text":"srun is another important command in SLURM. It allows you to run job steps directly without having to write a batch script. A job step is essentially a set of (possibly multiple) tasks that are co-scheduled across one or more nodes. In the context of our Pi estimation program, you could use srun to directly run the Pi estimation commands as a job step. However, as our script is a bit more complex (with loops and conditionals), it's not straightforward to do it with srun directly. Instead, we can wrap our core script into another script and then use srun to execute it: First, extract the core logic of our Pi estimation into a separate script, core_pi.sh : #!/bin/bash # Each task performs 10^6 trials trials=1000000 count=0 for (( j=0; j<trials; j++ )); do x=$(echo \"scale=4; $RANDOM/32767\" | bc -l) y=$(echo \"scale=4; $RANDOM/32767\" | bc -l) # Test if the point ($x, $y) lies within the unit circle inside=$( echo \"$x*$x + $y*$y < 1.0\" | bc -l) # Increase count if the point is inside the circle if [[ $inside -eq 1 ]]; then ((count++)) fi done # Estimate Pi based on the count and the number of trials pi=$(echo \"scale=4; 4*$count/$trials\" | bc -l) echo \"Task $SLURM_PROCID: Pi is approximately $pi\" Then, use srun to run this script as a job step, creating 4 tasks: srun --ntasks=4 --time=01:00:00 bash core_pi.sh Each of the 4 tasks will execute core_pi.sh separately, effectively running our Pi estimation in parallel. Remember, srun and salloc provide you with more flexibility and control over your job execution. You'll typically use sbatch for most of your jobs (especially long ones or ones that you want to schedule and forget), but srun and salloc can be very handy for quick or interactive jobs.","title":"Direct job step execution with srun"},{"location":"4_monitoring/","text":"Monitoring As we progress through our exploration of SLURM, we now approach an essential aspect of job management - monitoring. After you've submitted your job to the HPC cluster (like our Pi estimation job from the previous example), you'll want to track its status. Monitoring allows you to understand your job's progress, check its resource usage, and help identify any potential issues that could affect its successful completion. SLURM provides several tools to assist you in job monitoring, including squeue , sacct , sstat , and seff . Let's dive into these commands, understand their usage, and see how we can leverage them for efficient job tracking. Absolutely. Here's a more comprehensive version of the squeue section. squeue Think of squeue as your immediate source of information about your jobs. The squeue command displays information about jobs located in the SLURM scheduling queue. Let's say you've just submitted your Pi estimation job, and you're keen to check its status. Here's how to do it: squeue -u $USER The -u flag followed by $USER allows you to filter the jobs belonging to your user. The output will be a table listing your jobs: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 123456 debug pi_estim username R 0:25 1 node007 Let's decipher the output: JOBID : Unique identifier of your job in the SLURM system. PARTITION : The partition (or queue) where your job is placed. NAME : The name of your job. USER : The user who owns the job. ST : The state of your job ( R signifies that the job is currently running). TIME : The time your job has been running. NODES : The number of nodes your job is using. NODELIST(REASON) : The specific node(s) your job is running on. If you wish to customize the display and show specific fields, you can use the -o/--format option. For example, to display the job ID, name, state, and the number of CPUs: squeue -u $USER -o \"%.10i %.9P %.8j %.8u %.2t %.6D\" The format option %.10i %.9P %.8j %.8u %.2t %.6D is used to customize the output. Here's what each of these specifiers means: %.10i : Job ID with a field width of 10 characters. %.9P : Partition name with a field width of 9 characters. %.8j : Job name with a field width of 8 characters. %.8u : User name with a field width of 8 characters. %.2t : Job state with a field width of 2 characters. %.6D : Number of nodes with a field width of 6 characters. Field width specifies the minimum number of characters to be printed. If the value to be printed is shorter than this number, the result is padded with blank spaces. The full list of available options can be found in the SLURM documentation . With the squeue command, you can stay updated with the status and progress of your job in the scheduling queue. In the next section, we will explore sacct and sstat to get more detailed information about the resource usage of your job. Sure, let's dive into more ways you can use squeue to monitor your jobs. Example 1: Viewing All Jobs in a Specific Partition If you want to view all jobs currently in a specific partition, you can use the -p or --partition option. For example, to view all jobs in the regular partition: squeue -p regular This will show all jobs currently in the 'regular' partition, regardless of the user who submitted them. Example 2: Viewing Jobs in Specific States You can use squeue to display jobs in specific states. This can be particularly useful when you want to see how many jobs are running, pending, or completed. Use the -t or --states option followed by the state: squeue -u $USER -t RUNNING This command will show all your jobs currently running. Here are all the job states you can query using the -t or --states option of squeue : PENDING (or PD ): Job is awaiting resource allocation. Your job might be pending because the resources it needs are not currently available, or it might be waiting in line due to job scheduling policies. RUNNING (or R ): Job currently has an allocation and is running. SUSPENDED (or S ): Job has an allocation, but execution has been suspended and CPU usage has been reduced to zero, often due to some system event. COMPLETING (or CG ): Job is in the process of completing. Some processes of the job are still running. COMPLETED (or CD ): Job has completed successfully. CONFIGURING (or CF ): Job has been allocated resources, but are being configured for the job. CANCELLED (or CA ): Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. FAILED (or F ): Job terminated with non-zero exit code or other failure condition. TIMEOUT (or TO ): Job terminated upon reaching its time limit. PREEMPTED (or PR ): Job was preempted by a higher priority job. NODE_FAIL (or NF ): Job terminated due to failure of one or more allocated nodes. REVOKED (or RV ): Sibling job was unable to allocate resources and was revoked. SPECIAL_EXIT (or SE ): The job terminated in a condition that is interpreted as an exit code. Example 3: Sorting Jobs squeue can also sort jobs based on various attributes such as priority, job ID, time left, etc. For instance, to sort your jobs based on their remaining time in descending order, use the --sort option: squeue -u $USER --sort=-t This will display your jobs in descending order of remaining time ( -t ). The - sign before t is used for descending order. Without it, the jobs would be sorted in ascending order. These examples show the versatility of the squeue command and how it can be customized to get specific information about your jobs. Please remember to replace $USER with your username or keep it as is if you're running these commands directly in your terminal session. The full list of options is available in the SLURM documentation . Example 4: Showing Extended Job Information If you want to see more detailed information about a particular job, including its start time, estimated end time, and the nodes it's running on, you can use the -l or --long option. squeue -j 123456 --long This command will display extended information about the job with ID 123456 (replace it with your job ID). The output might look like this:< Tue May 23 10:15:39 2023 JOBID PARTITION NAME USER ST START END NODES NODELIST(REASON) 123456 test pi_estim myuser R 10:15:39 10:17:39 1 n0001 Example 5: Displaying Job Information in Parsable Format For scripts or other automated tasks, you might want to obtain the job information in a parsable format. The -h or --noheader option can be used to suppress the header, and the --Format option allows you to specify the exact fields you need. For instance: squeue -j 123456 --noheader --Format=jobid,username,state This will return a single line of output with the job ID, username, and state, separated by pipe characters. The output might look like: 123456|myuser|RUNNING In these examples, remember to replace 123456 with the ID of your job, and myuser with your username. The full list of format specifiers and options can be found in the SLURM documentation . sacct The sacct command provides accounting data for all jobs and job steps in the SLURM workload manager. It's a treasure trove of information about your job's performance, offering insights into resource usage and time spent on different stages. To view the accounting data for a specific job, use the -j flag followed by the job id: sacct -j 123456 --format=JobID,JobName,MaxRSS,Elapsed The --format option customizes the output, similar to the squeue command. In the above command, we are asking for the Job ID, Job Name, Maximum RSS memory used, and the Elapsed time of the job. A full list of available format options can be found in the SLURM documentation . The output might look like this: JobID JobName MaxRSS Elapsed ------------ ---------- ---------- ---------- 123456 pi_estim 10000K 00:02:00 sstat sstat allows us to fetch the real-time status of a running job or step. This command gives us live updates on resource usage, which can be essential for optimizing your application. To get real-time stats of your running job: sstat -j 123456 --format=JobID,AveCPU,AveRSS,AveVMSize Again, the --format option allows us to customize the output. This command displays the Job ID, average CPU usage, average resident set size (RSS) memory, and average virtual memory size. Here is a sample output: JobID AveCPU AveRSS AveVMSize --------- ---------- ---------- ----------- 123456 00:02:00 100.00MB 500.00MB The full list of format options can be found in the SLURM documentation . seff seff provides a brief summary of the efficiency of your job. Although it's not part of the default SLURM installation, it's widely used due to its effectiveness. seff 123456 This command will return something like this: Job ID: 123456 Cluster: cluster_name User/Group: user/group State: COMPLETED (exit code 0) Cores: 4 CPU Utilized: 00:08:00 CPU Efficiency: 100.00% of 00:08:00 core-walltime Wall-clock time: 00:02:00 Memory Utilized: 100.00 MB Memory Efficiency: 25.00% of 400.00 MB This output provides an overall picture of how efficiently your job utilized the allocated resources, which can be crucial in optimizing your application for an HPC environment. The use of these commands will give you a good understanding of how your job is performing and where there might be room for improvement. scontrol scontrol is a utility provided by SLURM for administrative tasks, but it also has several functions that can be useful for users. One of these is the ability to display detailed information about a specific job. For example: scontrol show job 123456 This command will show a detailed description of the job with ID 123456, including the time it was submitted, its current state, the partition it's running in, the resources it's using, and many other details. Here's a truncated example of what the output might look like: JobId=123456 JobName=pi_estim UserId=myuser(1000) GroupId=myuser(1000) MCS_label=N/A Priority=4294901496 Nice=0 Account=(null) QOS=(null) JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 ... Note: The output of scontrol show job is quite long and has been truncated for brevity. To see all the information, run this command yourself with one of your job IDs. sinfo While sinfo is more typically used to display information about nodes and partitions, it can also be used to monitor the resources being used by jobs in real-time. For example: sinfo -N -l This command will display a long format list of all nodes, showing which nodes are allocated to which jobs, the state of each node, how much of its resources are being used, and other details. Here's an example of what the output might look like: Tue May 23 10:15:39 2023 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON n0001 1 regular* allocated 24 2:12:1 64000 48000 1 (null) none JOBID PARTITION NAME USER ST START END NODES NODELIST(REASON) 123456 regular pi_estim myuser R 10:15:39 10:17:39 1 n0001 In this example, you can see that node n0001 is allocated to job 123456, which is currently running. As always, remember to replace 123456 with your job ID and myuser with your username in these examples. And as always, the full list of options is available in the SLURM documentation .","title":"Monitoring"},{"location":"4_monitoring/#monitoring","text":"As we progress through our exploration of SLURM, we now approach an essential aspect of job management - monitoring. After you've submitted your job to the HPC cluster (like our Pi estimation job from the previous example), you'll want to track its status. Monitoring allows you to understand your job's progress, check its resource usage, and help identify any potential issues that could affect its successful completion. SLURM provides several tools to assist you in job monitoring, including squeue , sacct , sstat , and seff . Let's dive into these commands, understand their usage, and see how we can leverage them for efficient job tracking. Absolutely. Here's a more comprehensive version of the squeue section.","title":"Monitoring"},{"location":"4_monitoring/#squeue","text":"Think of squeue as your immediate source of information about your jobs. The squeue command displays information about jobs located in the SLURM scheduling queue. Let's say you've just submitted your Pi estimation job, and you're keen to check its status. Here's how to do it: squeue -u $USER The -u flag followed by $USER allows you to filter the jobs belonging to your user. The output will be a table listing your jobs: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 123456 debug pi_estim username R 0:25 1 node007 Let's decipher the output: JOBID : Unique identifier of your job in the SLURM system. PARTITION : The partition (or queue) where your job is placed. NAME : The name of your job. USER : The user who owns the job. ST : The state of your job ( R signifies that the job is currently running). TIME : The time your job has been running. NODES : The number of nodes your job is using. NODELIST(REASON) : The specific node(s) your job is running on. If you wish to customize the display and show specific fields, you can use the -o/--format option. For example, to display the job ID, name, state, and the number of CPUs: squeue -u $USER -o \"%.10i %.9P %.8j %.8u %.2t %.6D\" The format option %.10i %.9P %.8j %.8u %.2t %.6D is used to customize the output. Here's what each of these specifiers means: %.10i : Job ID with a field width of 10 characters. %.9P : Partition name with a field width of 9 characters. %.8j : Job name with a field width of 8 characters. %.8u : User name with a field width of 8 characters. %.2t : Job state with a field width of 2 characters. %.6D : Number of nodes with a field width of 6 characters. Field width specifies the minimum number of characters to be printed. If the value to be printed is shorter than this number, the result is padded with blank spaces. The full list of available options can be found in the SLURM documentation . With the squeue command, you can stay updated with the status and progress of your job in the scheduling queue. In the next section, we will explore sacct and sstat to get more detailed information about the resource usage of your job. Sure, let's dive into more ways you can use squeue to monitor your jobs.","title":"squeue"},{"location":"4_monitoring/#example-1-viewing-all-jobs-in-a-specific-partition","text":"If you want to view all jobs currently in a specific partition, you can use the -p or --partition option. For example, to view all jobs in the regular partition: squeue -p regular This will show all jobs currently in the 'regular' partition, regardless of the user who submitted them.","title":"Example 1: Viewing All Jobs in a Specific Partition"},{"location":"4_monitoring/#example-2-viewing-jobs-in-specific-states","text":"You can use squeue to display jobs in specific states. This can be particularly useful when you want to see how many jobs are running, pending, or completed. Use the -t or --states option followed by the state: squeue -u $USER -t RUNNING This command will show all your jobs currently running. Here are all the job states you can query using the -t or --states option of squeue : PENDING (or PD ): Job is awaiting resource allocation. Your job might be pending because the resources it needs are not currently available, or it might be waiting in line due to job scheduling policies. RUNNING (or R ): Job currently has an allocation and is running. SUSPENDED (or S ): Job has an allocation, but execution has been suspended and CPU usage has been reduced to zero, often due to some system event. COMPLETING (or CG ): Job is in the process of completing. Some processes of the job are still running. COMPLETED (or CD ): Job has completed successfully. CONFIGURING (or CF ): Job has been allocated resources, but are being configured for the job. CANCELLED (or CA ): Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. FAILED (or F ): Job terminated with non-zero exit code or other failure condition. TIMEOUT (or TO ): Job terminated upon reaching its time limit. PREEMPTED (or PR ): Job was preempted by a higher priority job. NODE_FAIL (or NF ): Job terminated due to failure of one or more allocated nodes. REVOKED (or RV ): Sibling job was unable to allocate resources and was revoked. SPECIAL_EXIT (or SE ): The job terminated in a condition that is interpreted as an exit code.","title":"Example 2: Viewing Jobs in Specific States"},{"location":"4_monitoring/#example-3-sorting-jobs","text":"squeue can also sort jobs based on various attributes such as priority, job ID, time left, etc. For instance, to sort your jobs based on their remaining time in descending order, use the --sort option: squeue -u $USER --sort=-t This will display your jobs in descending order of remaining time ( -t ). The - sign before t is used for descending order. Without it, the jobs would be sorted in ascending order. These examples show the versatility of the squeue command and how it can be customized to get specific information about your jobs. Please remember to replace $USER with your username or keep it as is if you're running these commands directly in your terminal session. The full list of options is available in the SLURM documentation .","title":"Example 3: Sorting Jobs"},{"location":"4_monitoring/#example-4-showing-extended-job-information","text":"If you want to see more detailed information about a particular job, including its start time, estimated end time, and the nodes it's running on, you can use the -l or --long option. squeue -j 123456 --long This command will display extended information about the job with ID 123456 (replace it with your job ID). The output might look like this:< Tue May 23 10:15:39 2023 JOBID PARTITION NAME USER ST START END NODES NODELIST(REASON) 123456 test pi_estim myuser R 10:15:39 10:17:39 1 n0001","title":"Example 4: Showing Extended Job Information"},{"location":"4_monitoring/#example-5-displaying-job-information-in-parsable-format","text":"For scripts or other automated tasks, you might want to obtain the job information in a parsable format. The -h or --noheader option can be used to suppress the header, and the --Format option allows you to specify the exact fields you need. For instance: squeue -j 123456 --noheader --Format=jobid,username,state This will return a single line of output with the job ID, username, and state, separated by pipe characters. The output might look like: 123456|myuser|RUNNING In these examples, remember to replace 123456 with the ID of your job, and myuser with your username. The full list of format specifiers and options can be found in the SLURM documentation .","title":"Example 5: Displaying Job Information in Parsable Format"},{"location":"4_monitoring/#sacct","text":"The sacct command provides accounting data for all jobs and job steps in the SLURM workload manager. It's a treasure trove of information about your job's performance, offering insights into resource usage and time spent on different stages. To view the accounting data for a specific job, use the -j flag followed by the job id: sacct -j 123456 --format=JobID,JobName,MaxRSS,Elapsed The --format option customizes the output, similar to the squeue command. In the above command, we are asking for the Job ID, Job Name, Maximum RSS memory used, and the Elapsed time of the job. A full list of available format options can be found in the SLURM documentation . The output might look like this: JobID JobName MaxRSS Elapsed ------------ ---------- ---------- ---------- 123456 pi_estim 10000K 00:02:00","title":"sacct"},{"location":"4_monitoring/#sstat","text":"sstat allows us to fetch the real-time status of a running job or step. This command gives us live updates on resource usage, which can be essential for optimizing your application. To get real-time stats of your running job: sstat -j 123456 --format=JobID,AveCPU,AveRSS,AveVMSize Again, the --format option allows us to customize the output. This command displays the Job ID, average CPU usage, average resident set size (RSS) memory, and average virtual memory size. Here is a sample output: JobID AveCPU AveRSS AveVMSize --------- ---------- ---------- ----------- 123456 00:02:00 100.00MB 500.00MB The full list of format options can be found in the SLURM documentation .","title":"sstat"},{"location":"4_monitoring/#seff","text":"seff provides a brief summary of the efficiency of your job. Although it's not part of the default SLURM installation, it's widely used due to its effectiveness. seff 123456 This command will return something like this: Job ID: 123456 Cluster: cluster_name User/Group: user/group State: COMPLETED (exit code 0) Cores: 4 CPU Utilized: 00:08:00 CPU Efficiency: 100.00% of 00:08:00 core-walltime Wall-clock time: 00:02:00 Memory Utilized: 100.00 MB Memory Efficiency: 25.00% of 400.00 MB This output provides an overall picture of how efficiently your job utilized the allocated resources, which can be crucial in optimizing your application for an HPC environment. The use of these commands will give you a good understanding of how your job is performing and where there might be room for improvement.","title":"seff"},{"location":"4_monitoring/#scontrol","text":"scontrol is a utility provided by SLURM for administrative tasks, but it also has several functions that can be useful for users. One of these is the ability to display detailed information about a specific job. For example: scontrol show job 123456 This command will show a detailed description of the job with ID 123456, including the time it was submitted, its current state, the partition it's running in, the resources it's using, and many other details. Here's a truncated example of what the output might look like: JobId=123456 JobName=pi_estim UserId=myuser(1000) GroupId=myuser(1000) MCS_label=N/A Priority=4294901496 Nice=0 Account=(null) QOS=(null) JobState=RUNNING Reason=None Dependency=(null) Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 ... Note: The output of scontrol show job is quite long and has been truncated for brevity. To see all the information, run this command yourself with one of your job IDs.","title":"scontrol"},{"location":"4_monitoring/#sinfo","text":"While sinfo is more typically used to display information about nodes and partitions, it can also be used to monitor the resources being used by jobs in real-time. For example: sinfo -N -l This command will display a long format list of all nodes, showing which nodes are allocated to which jobs, the state of each node, how much of its resources are being used, and other details. Here's an example of what the output might look like: Tue May 23 10:15:39 2023 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON n0001 1 regular* allocated 24 2:12:1 64000 48000 1 (null) none JOBID PARTITION NAME USER ST START END NODES NODELIST(REASON) 123456 regular pi_estim myuser R 10:15:39 10:17:39 1 n0001 In this example, you can see that node n0001 is allocated to job 123456, which is currently running. As always, remember to replace 123456 with your job ID and myuser with your username in these examples. And as always, the full list of options is available in the SLURM documentation .","title":"sinfo"},{"location":"5_constrains/","text":"Policies and constrains In an HPC (High-Performance Computing) environment, imposing constraints plays a crucial role in effectively managing and optimizing job execution. These constraints collectively contribute to the overall effectiveness and fairness of job management in an HPC environment. By implementing appropriate constraints, HPC systems can enhance resource utilization, minimize job delays, and accommodate the diverse computational needs of multiple users and applications. QoS/Partition Priority MaxWall MaxNodesPU MaxJobsPU MaxSubmitPU MaxTRES regular 200 1-00:00:00 24 50 test 500 00:10:00 2 2 2 long 200 2-00:00:00 24 20 xlong 200 8-00:00:00 6 14 large 200 2-00:00:00 40 6 xlarge 200 2-00:00:00 80 6 serial 200 2-00:00:00 24 120 cpu=1 gpu=1 node=1 This is what each columns means: MaxWall: Maximum amount of time the job is allowed to run. \u00b41-00:00:00\u00b4 reads as one day or 24 hours. MaxNodesPU: Maximum amount of nodes user's jobs can use at a given time. MaxJobsPU: Maximum number of running jobs per user. MaxSubmitPU: Maximum number of jobs that can be submitted to the QoS/partition. MaxTRES: The maximum number of Trackable RESources (TRES) each job is able to use.","title":"Policies and constrains"},{"location":"5_constrains/#policies-and-constrains","text":"In an HPC (High-Performance Computing) environment, imposing constraints plays a crucial role in effectively managing and optimizing job execution. These constraints collectively contribute to the overall effectiveness and fairness of job management in an HPC environment. By implementing appropriate constraints, HPC systems can enhance resource utilization, minimize job delays, and accommodate the diverse computational needs of multiple users and applications. QoS/Partition Priority MaxWall MaxNodesPU MaxJobsPU MaxSubmitPU MaxTRES regular 200 1-00:00:00 24 50 test 500 00:10:00 2 2 2 long 200 2-00:00:00 24 20 xlong 200 8-00:00:00 6 14 large 200 2-00:00:00 40 6 xlarge 200 2-00:00:00 80 6 serial 200 2-00:00:00 24 120 cpu=1 gpu=1 node=1 This is what each columns means: MaxWall: Maximum amount of time the job is allowed to run. \u00b41-00:00:00\u00b4 reads as one day or 24 hours. MaxNodesPU: Maximum amount of nodes user's jobs can use at a given time. MaxJobsPU: Maximum number of running jobs per user. MaxSubmitPU: Maximum number of jobs that can be submitted to the QoS/partition. MaxTRES: The maximum number of Trackable RESources (TRES) each job is able to use.","title":"Policies and constrains"},{"location":"6_postrun/","text":"Post-Run Operations Once the work is finished we should move the generated data to our home directory under /dipc or to a local folder. This is done for two main reasons: The scrath is not backed up , so in case there is a problem with the filesystem, the stored data will be lost. The /scratch file system is designed for performance rather than reliability. When the occupancy goes above 80% the BeeGFS filesystem shows a performance degradation that affects all users . Analyzing job performance with seff SLURM provides a tool called seff to check the memory utilization and CPU efficiency for completed jobs. Note that for running and failed jobs, the efficiency numbers reported by seff are not reliable so please use this tool only for successfully completed jobs: seff <job_id>","title":"Post Run"},{"location":"6_postrun/#post-run-operations","text":"Once the work is finished we should move the generated data to our home directory under /dipc or to a local folder. This is done for two main reasons: The scrath is not backed up , so in case there is a problem with the filesystem, the stored data will be lost. The /scratch file system is designed for performance rather than reliability. When the occupancy goes above 80% the BeeGFS filesystem shows a performance degradation that affects all users .","title":"Post-Run Operations"},{"location":"6_postrun/#analyzing-job-performance-with-seff","text":"SLURM provides a tool called seff to check the memory utilization and CPU efficiency for completed jobs. Note that for running and failed jobs, the efficiency numbers reported by seff are not reliable so please use this tool only for successfully completed jobs: seff <job_id>","title":"Analyzing job performance with seff"},{"location":"7_examples/","text":"Examples Here we collect different usefull sbatch script examples. .info-box { background-color: #f0f8ff; padding: 20px; border: 1px solid #e6eaf2; border-radius: 4px; margin-bottom: 20px; font-family: Courier, monospace; } .info-box h3 { font-size: 20px; margin-bottom: 10px; color: #0085ff; cursor: pointer; font-family: Helvetica, sans-serif; /* Set your desired regular font here */ } .info-box p { font-size: 16px; line-height: 1.5; color: #333; font-family: Courier, monospace; } .info-box .content { display: none; /* Collapsed by default */ } Serial Serial jobs are tasks that run sequentially on a single processor without parallelization. They are used for workloads that can't be easily parallelized or don't benefit from parallel processing. #!/bin/bash #SBATCH --job-name=serial_job #SBATCH --output=output.log #SBATCH --error=error.log #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 echo \"Running serial job...\" # Your serial job commands go here MPI MPI (Message Passing Interface) is a parallel programming model for distributed memory systems. It enables programs to run across multiple processors, communicating via message passing. #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=JOB_NAME #SBATCH --cpus-per-task=1 #SBATCH --mem=200gb #SBATCH --nodes=8 #SBATCH --ntasks-per-node=48 module load program/program_version mpirun -np $SLURM_NTASKS binaryi < input OpenMP For a OpenMP application the number of threads can be controlled defining the ``OMP_NUM_THREADS`` or SLURM's ``--cpus-per-task`` job directive. If this variable is not defined, the number of threads created will be equal to the amount of cores reserved in your cpuset, that is, the number of cores requested in the batch script. #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=JOB_NAME #SBATCH --cpus-per-task=48 #SBATCH --mem=200gb #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 module load program/program_version export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK binary < input Hybrid (MPI+OpenMP) Hybrid parallelization, combining MPI (Message Passing Interface) and OpenMP, is a powerful approach for harnessing the computational capabilities of both distributed and shared memory systems. In this paradigm, MPI is used for inter-node communication, enabling data exchange and synchronization between distributed processes, while OpenMP is employed within each node for intra-node parallelization across multiple threads. #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=JOB_NAME #SBATCH --cpus-per-task=4 #SBATCH --mem=200gb #SBATCH --nodes=2 #SBATCH --ntasks-per-node=12 module load program/program_version export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK mpirun -np $SLURM_NTASKS binaryi < input GPU jobs GPU jobs refer to tasks or applications that utilize the computational power of Graphics Processing Units (GPUs) for accelerated processing. GPU jobs are commonly used for deep learning, scientific simulations, data analytics, and other computationally intensive tasks that can benefit from parallel processing on GPUs. By leveraging the power of GPUs, these jobs can achieve significant performance gains compared to running on CPUs alone.One can request the usage of GPUs by adding ``#SBATCH --gres=gpu:p40:X`` to the submision script. In the following example we request 2 GPUs per node #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=GROMACS_job #SBATCH --mem=200gb #SBATCH --cpus-per-task=1 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=8 #SBATCH --gres=gpu:p40:2 #SBATCH --output=%x-%j.out #SBATCH --error=%x-%j.err module load GROMACS/2020-fosscuda-2019b srun gmx_mpi mdrun -ntomp $SLURM_CPUS_PER_TASK -nb auto -bonded auto -pme auto -gpu_id 01 -s input.tpr Job Array SLURM job arrays allow users to submit and manage a group of related jobs as a single entity. A job array consists of multiple tasks that are similar in nature but have different input data or parameters. SLURM handles the task distribution, resource allocation, and job dependencies automatically. They simplify job submission and management, improve efficiency, and provide better control over large-scale job execution in HPC environments. #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=ARRAY_JOB #SBATCH --time=00:10:00 #SBATCH --nodes=1 # nodes per instance #SBATCH --ntasks=1 # tasks per instance #SBATCH --array=0-9 # instance indexes #SBATCH --output=%x-%j.out #SBATCH --error=%x-%j.err echo \"Slurm job id is ${SLURM_JOB_ID}\" echo \"Array job id is ${SLURM_ARRAY_JOB_ID}\" echo \"Instance index is ${SLURM_ARRAY_TASK_ID}.\" Dependency chains Job dependencies are used to defer the start of a job until some dependencies have been satisfied. Job dependencies can be defined using the ``--dependency`` argument of the ``sbatch`` command: ``#SBATCH --dependency=\"dependency_type\"`` Available dependencies are: - ``after:jobID`` job starts when job with ``jobID`` begun execution. - ``afterany:jobID`` job starts when job with ``jobID`` terminates. - ``aferok:jobID`` job starts when job with ``jobID`` terminates successfully. - ``afternook:jobID`` job starts when job with ``jobID`` terminates with non-zero status. - ``singleton:jobID`` jobs starts when any previously job with the same job name and user terminates. This can be used to chain restartable jobs. #!/bin/bash #SBATCH --partition=regular #SBATCH --job-name=ARRAY_JOB #SBATCH --time=00:10:00 #SBATCH --nodes=1 # nodes per instance #SBATCH --ntasks=1 # tasks per instance #SBATCH --array=0-9 # instance indexes #SBATCH --output=%x-%j.out #SBATCH --error=%x-%j.err echo \"Slurm job id is ${SLURM_JOB_ID}\" echo \"Array job id is ${SLURM_ARRAY_JOB_ID}\" echo \"Instance index is ${SLURM_ARRAY_TASK_ID}.\" function toggleInfoBox(element) { var content = element.nextElementSibling; if (content.style.display === 'none' || content.style.display === '') { content.style.display = 'block'; } else { content.style.display = 'none'; } }","title":"Examplex"},{"location":"7_examples/#examples","text":"Here we collect different usefull sbatch script examples. .info-box { background-color: #f0f8ff; padding: 20px; border: 1px solid #e6eaf2; border-radius: 4px; margin-bottom: 20px; font-family: Courier, monospace; } .info-box h3 { font-size: 20px; margin-bottom: 10px; color: #0085ff; cursor: pointer; font-family: Helvetica, sans-serif; /* Set your desired regular font here */ } .info-box p { font-size: 16px; line-height: 1.5; color: #333; font-family: Courier, monospace; } .info-box .content { display: none; /* Collapsed by default */ }","title":"Examples"},{"location":"8_advanced/","text":"Advanced Scalability experiments Scaling experiments are crucial for assessing code scalability and parallel performance. They involve systematically varying computational resources to gain insights into code behavior under different workloads. These experiments provide valuable information, identify limitations, and drive improvements. Results serve two key purposes. Firstly, they diagnose code performance by analyzing execution times and speedup as resources increase. This helps identify bottlenecks and optimize code for better performance. Secondly, scaling results are necessary for resource allocation requests. Funding agencies require evidence of code scalability and performance to assess resource requirements and potential impact. Visualizations like speedup and efficiency plots effectively present scaling results. They illustrate execution time improvement and resource utilization effectiveness. In conclusion, scaling experiments are vital for understanding code scalability and parallel performance. They inform optimization efforts and support resource allocation requests. Visualizations aid in communicating results effectively. .info-box { background-color: #f0f8ff; padding: 20px; border: 1px solid #e6eaf2; border-radius: 4px; margin-bottom: 20px; } .info-box h3 { font-size: 20px; margin-bottom: 10px; color: #0085ff; } .info-box p { font-size: 16px; line-height: 1.5; color: #333; } .info-box .additional-info { margin-top: 20px; padding: 10px; background-color: #f9f9f9; border: 1px solid #e6eaf2; border-radius: 4px; display: none; /* Collapsed by default */ } .info-box .additional-info-toggle { cursor: pointer; color: #0085ff; font-weight: bold; text-decoration: underline; } Determine best performance from a scalability study Consider the following scalability plot for a random application. ![Scalability](img/scalability_study.png) At what point would you consider to be peak performance in this example. The point where performance gains are no longer linear The apex of the curve The maximum core count None of the above You may find that a scalability graph may vary if you ran the same code on a different machine. Why? Show Solution No, the performance is still increasing, at this point we are no longer achieving perfect scalability. Yes, the performance peaks at this location, and one cannot get higher speed up with this set up. No, peak performance has already been achieved, and increasing the core count will onlt reduce performance. No, although you can run extra benchmarks to find the exact number of cores at which the inflection point truly lies, there is no real purpose for doing so. Tying into the answer for #4, if you produce scalability studies on different machines, they will be different because of the different setup, hardware of the machine. You are never going to get two scalability studies which are identical, but they will agree to some point. var additionalInfoToggle = document.querySelector('.additional-info-toggle'); var additionalInfo = document.querySelector('.additional-info'); additionalInfoToggle.addEventListener('click', function() { if (additionalInfo.style.display === 'none' || additionalInfo.style.display === '') { additionalInfo.style.display = 'block'; additionalInfoToggle.textContent = 'Hide Solution'; } else { additionalInfo.style.display = 'none'; additionalInfoToggle.textContent = 'Show Solution'; } });","title":"Advanced"},{"location":"8_advanced/#advanced","text":"","title":"Advanced"},{"location":"8_advanced/#scalability-experiments","text":"Scaling experiments are crucial for assessing code scalability and parallel performance. They involve systematically varying computational resources to gain insights into code behavior under different workloads. These experiments provide valuable information, identify limitations, and drive improvements. Results serve two key purposes. Firstly, they diagnose code performance by analyzing execution times and speedup as resources increase. This helps identify bottlenecks and optimize code for better performance. Secondly, scaling results are necessary for resource allocation requests. Funding agencies require evidence of code scalability and performance to assess resource requirements and potential impact. Visualizations like speedup and efficiency plots effectively present scaling results. They illustrate execution time improvement and resource utilization effectiveness. In conclusion, scaling experiments are vital for understanding code scalability and parallel performance. They inform optimization efforts and support resource allocation requests. Visualizations aid in communicating results effectively. .info-box { background-color: #f0f8ff; padding: 20px; border: 1px solid #e6eaf2; border-radius: 4px; margin-bottom: 20px; } .info-box h3 { font-size: 20px; margin-bottom: 10px; color: #0085ff; } .info-box p { font-size: 16px; line-height: 1.5; color: #333; } .info-box .additional-info { margin-top: 20px; padding: 10px; background-color: #f9f9f9; border: 1px solid #e6eaf2; border-radius: 4px; display: none; /* Collapsed by default */ } .info-box .additional-info-toggle { cursor: pointer; color: #0085ff; font-weight: bold; text-decoration: underline; }","title":"Scalability experiments"},{"location":"about/","text":"Humanum veluti corpus Flores silva Lorem markdownum dispar laborant cubitique mutataeque animo . Ostendit Eumenides esse quae positaeque rogum Atreus quanto arma properandum. Ille turis, si vellera nympha. Uteri suspirat latus , non dies loqui huic. Regnat sed vincta gramine: bella indignatus erudit Atlantis, ad paret. commandLifoSimm *= -3; if (3 == moodleBar) { payload += bar; } in_logic_cyberspace = rss_it_dongle; bluetooth_core(-4); Vires residens pastoribus rapuere Ceres simulat per mente, mihi aequora his parce dea neque ad nubes. Luctantiaque orbe advertens numeros certe praestantia prolem quoque; pater intus. Telis rubentem. Fera forte fertur desubito Tellus et barbara Dardanidas albis vivit profecturas superas lacteus Pelates ignavus o cernunt animus manibus tradit ipse. Non sui illius amanti est pennis faveas, oraque, alvo. Pater accessisse et insigne aquas aliae spectemur castra fatebar iamque profanus Niobe concutit candescere letum et in est . Amat regemque inque Molpeus putares non viribus vidi docet, sum, Hectoris tum. Reliquit est antris ausa mites nudis, tot vidi rumorum Oenides iacentes filia cupies vulnera. Et Stheneleius marem recumbis, antiquum lucum. Ita idem merito labor e redeuntia iacet. Vagantem abest Vultu volenti inportuna aevi, mollis pectusque manu Pagasaeus legebant vulnera caelestique ergo adclinavit admoveam et . At nuper, in me temptanda litora ingestoque micat cur Telamone fugit. Dimovit ipse , qua duceret; ite rapta laquei est minus viribus rabiem in teneo adit. var newbieBrowserType = 20; var algorithm = parameter_mouse_post; if (soapWindowsDrive <= lamp_eps_clip * copy) { default_drive = 5 - 1; } var point = jpeg_white; gpu_ripcording = cellDirect.clockHitSpeakers(social.emoticon(desktop, 32, dhcp)) / lanArtificialState; Tu enim illi quoque, purpura spargit supponere illa. Mutant dignamque an dolor roganti texitur, fata silva tincta submissa, consederat.","title":"About"},{"location":"about/#humanum-veluti-corpus","text":"","title":"Humanum veluti corpus"},{"location":"about/#flores-silva","text":"Lorem markdownum dispar laborant cubitique mutataeque animo . Ostendit Eumenides esse quae positaeque rogum Atreus quanto arma properandum. Ille turis, si vellera nympha. Uteri suspirat latus , non dies loqui huic. Regnat sed vincta gramine: bella indignatus erudit Atlantis, ad paret. commandLifoSimm *= -3; if (3 == moodleBar) { payload += bar; } in_logic_cyberspace = rss_it_dongle; bluetooth_core(-4);","title":"Flores silva"},{"location":"about/#vires-residens-pastoribus-rapuere","text":"Ceres simulat per mente, mihi aequora his parce dea neque ad nubes. Luctantiaque orbe advertens numeros certe praestantia prolem quoque; pater intus. Telis rubentem.","title":"Vires residens pastoribus rapuere"},{"location":"about/#fera-forte-fertur-desubito-tellus-et-barbara","text":"Dardanidas albis vivit profecturas superas lacteus Pelates ignavus o cernunt animus manibus tradit ipse. Non sui illius amanti est pennis faveas, oraque, alvo. Pater accessisse et insigne aquas aliae spectemur castra fatebar iamque profanus Niobe concutit candescere letum et in est . Amat regemque inque Molpeus putares non viribus vidi docet, sum, Hectoris tum. Reliquit est antris ausa mites nudis, tot vidi rumorum Oenides iacentes filia cupies vulnera. Et Stheneleius marem recumbis, antiquum lucum. Ita idem merito labor e redeuntia iacet.","title":"Fera forte fertur desubito Tellus et barbara"},{"location":"about/#vagantem-abest","text":"Vultu volenti inportuna aevi, mollis pectusque manu Pagasaeus legebant vulnera caelestique ergo adclinavit admoveam et . At nuper, in me temptanda litora ingestoque micat cur Telamone fugit. Dimovit ipse , qua duceret; ite rapta laquei est minus viribus rabiem in teneo adit. var newbieBrowserType = 20; var algorithm = parameter_mouse_post; if (soapWindowsDrive <= lamp_eps_clip * copy) { default_drive = 5 - 1; } var point = jpeg_white; gpu_ripcording = cellDirect.clockHitSpeakers(social.emoticon(desktop, 32, dhcp)) / lanArtificialState; Tu enim illi quoque, purpura spargit supponere illa. Mutant dignamque an dolor roganti texitur, fata silva tincta submissa, consederat.","title":"Vagantem abest"}]}